---
title: "stitches in R"
author: "ACS"
date: '2022-09-22'
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# IMPORTANT NOTES

- Necessary to avoid R variable names like `variable.name` since `.` in python is somewhat analogous to `$` in R. 
- Follow the instructions in the `stitches-in-R-setup` R markdown so that this notebook will work. If you've followed that markdown, `stitches` should be installed in the `r-reticulate` virtual environment for this notebook and should be callable. 
- there's some redundancies on my python imports, I should put in one place and clean up at some point.

- gridded stitching does slow things down. This notebook takes maybe 15 seconds to knit when there's no gridded calculations. Adding them increases the knit time to maybe like 5 minutes. 

- can add `eval=FALSE` for those blocks so people can get a first knit notebook quickly (all towards end).

# Setup - `reticulate` 

```{r}

library(reticulate)

# # indicate that we want to use a specific virtualenv
use_virtualenv("r-reticulate", required =TRUE)
```
## quick `reticulate` test

quick block of code that will only successfully knit if the setup was done correctly
```{python}
import numpy as np
print(np.max([2,3]))
```

## make sure can import stitches and call a function from it

This is a good function to test a `stitches` installation with - no arguments needed, just a list of data on pangeo, implicitly tests that can connect to pangeo. 

```{python}
import pandas as pd
pd.set_option('display.max_columns', None)

import stitches
pangeo_table = stitches.fx_pangeo.fetch_pangeo_table()
print(pangeo_table.head())
```


# Setup - R packages
```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

```

# Load Hector data
pulled a quick rcp4.5 from HectorUI
```{r}
hector_tgav <- read.csv('data/Hector-data-2022-09-22.csv', skip=3, stringsAsFactors = FALSE)
hector_exp <- unique(hector_tgav$scenario)
head(hector_tgav)
```

# Make sure we can work with it from a python block
This is also a quick back and forth of moving data between R and python blocks following:

- https://rstudio.github.io/reticulate/articles/r_markdown.html#calling-python-from-r
- https://rstudio.github.io/reticulate/articles/r_markdown.html#calling-r-from-python



```{python}
import numpy as np
import pandas as pd

tgav_df = pd.DataFrame(r.hector_tgav)
print(np.max(tgav_df[['value']]))

```

and we can compare to the R evaluation
```{r}
# The hector data we read straight in to R
print(max(hector_tgav$value))

# R operations on the python data
print(max(py$tgav_df$value))
```


# Reshape the Hector data to be stitches target

First we reshape the Hector time series to match the CMIP6 ESM GSAT smooth anomaly time series that STITCHES operates on:
NOTE we should probably add this as a function to stitches 
NOTE Hector time series start in 1800 - there's no science reason we couldn't do matching over that window and have a gridded netcdf of an extra 50 years of history. 
```{r}
hector_tgav %>%
    mutate(variable = 'tas',
           ensemble = 'hectorUI1',# doesn't affect the matching or stitching
           model = 'Hector') %>%
    rename(experiment = scenario) %>% 
    filter(year >= 1850) %>%
    select(variable, experiment, ensemble, model, year, value,  -units) ->
    target_tgav



print(head(target_tgav))
```
# Convert Hector Tgav to stitches windows

Now that we have shaped it as stitches is expecting, we can use stitches functions to calculate the matching windows for this target data

```{python}
import stitches 
import pandas as pd
import pkg_resources
import xarray as xr
import numpy as np
pd.set_option('display.max_columns', None)

# smooth out Hector's historic volcano stuff and
# save off the smoothed Hector time series we actually use for matching so can plot later:      
target_hector = stitches.fx_processing.calculate_rolling_mean(r.target_tgav, size =19)

target_data = stitches.fx_processing.get_chunk_info(
    stitches.fx_processing.chunk_ts(
      df = target_hector,
      n=9))
      
print(target_data.head())
```

# Load the stitches archive data for matching

## variables and experiments of interest

We want to make sure that when we match to a point in an archive, that point has every netcdfs logged for every variable we care about. 


```{python}
esm = ['CanESM5']
vars1 = ['tas', 'pr', 'hurs', 'psl']
exps = ['ssp126', 'ssp245', 'ssp370', 'ssp460', 'ssp585']
# pangeo table of ESMs for reference
pangeo_path = pkg_resources.resource_filename('stitches', 'data/pangeo_table.csv')
pangeo_data = pd.read_csv(pangeo_path)
# print(np.sort(pangeo_data.variable.unique()))
pangeo_data = pangeo_data[((
                               (pangeo_data['variable'].isin(vars1)) )  )
                          & ((pangeo_data['domain'].str.contains('mon')) ) &
                           ((pangeo_data['experiment'].isin(exps))) &
                           (pangeo_data['model'].isin(esm))].copy()



pangeo_good_ensembles =[]
for name, group in pangeo_data.groupby(['model', 'experiment', 'ensemble']):
    df = group.drop_duplicates().copy()
    if len(df) >= len(vars1):
        pangeo_good_ensembles.append(df)
    del(df)
pangeo_good_ensembles1 = pd.concat(pangeo_good_ensembles)
pangeo_good_ensembles2  = pangeo_good_ensembles1[['model', 'experiment', 'ensemble']].drop_duplicates().copy()
pangeo_good_ensembles = pangeo_good_ensembles2.reset_index(drop=True).copy()
del(pangeo_good_ensembles1)
del(pangeo_good_ensembles2)
```



## subset archive appropriately


```{python}
path = pkg_resources.resource_filename('stitches', 'data/matching_archive.csv')
archive_data = pd.read_csv(path)


# Keep only the entries that appeared in pangeo_good_ensembles:
keys =['model', 'experiment', 'ensemble']
i1 = archive_data.set_index(keys).index
i2 = pangeo_good_ensembles.set_index(keys).index
archive_data= archive_data[i1.isin(i2)].copy()
del(i1)
del(i2)

# # Don't keep archive points with a base year of every year, just the defaults and
# # the midpont
# # do the subset to just core years + 1/2 window offset years.
# years = np.concatenate((1854+9*np.array(list( range(0,28))), 1859+9*np.array(list( range(0,27)))))
# archive_data = archive_data[archive_data['year'].isin(years)].reset_index(drop=True).copy()

print(archive_data.head())

```

# Match
```{python}

my_recipes = stitches.make_recipe(target_data, archive_data,
                                  non_tas_variables=['pr', 'psl', 'hurs'],
                                  tol = 0.14, N_matches = 10000, reproducible = True)

print(my_recipes.head())
```

Unless otherwise specified by the argument `reproducible = True`, the draws stitches takes from the pool of matches is stochastic. Do another draw to show off:

```{python}

my_recipes2 = stitches.make_recipe(target_data, archive_data,
                                  non_tas_variables=['pr', 'psl', 'hurs'],
                                  tol = 0.14, N_matches = 10000)

```
Within each of these ensembles, there's no envelope collapse. If we were to concatenate them together into
a super-ensemble, there would be. So it's not as powerful as a very large ensemble of collapse free realizations but it's still a distinct ensemble to consider.



# stitch Tgav
```{python}
stitched_global_temp = stitches.gmat_stitching(my_recipes)
print(stitched_global_temp.head())
```


```{python}
stitched_global_temp2 = stitches.gmat_stitching(my_recipes2)

```
# plot Tgav
```{r, echo = FALSE}
stitched_tgav <- py$stitched_global_temp

# some reshaping to make plotting easier
stitched_tgav %>%
    mutate(tmp = stitching_id) %>%
    separate(tmp, into = c('scenario', 't1', 't2'), sep = '~') %>%
    select(-t1, -t2) ->
    stitched_tgav

ggplot(data = stitched_tgav, aes(x= year, y = value, color = stitching_id)) +
    geom_line(size=0.2) +
  geom_line(data = hector_tgav, aes(x = year, y=value), color='black', size = 1.1) + 
  geom_line(data = py$target_hector, aes(x = year, y=value), color='blue', size = 0.7) + 
  ylab('degC') +
  ylab('degC') +
  ggtitle('global average temperature anomaly - Ensemble 1')+ 
  theme(legend.position="bottom") +
  theme(legend.title = element_text( size=4), legend.text=element_text(size=4))




stitched_tgav2 <- py$stitched_global_temp2

# some reshaping to make plotting easier
stitched_tgav2 %>%
    mutate(tmp = stitching_id) %>%
    separate(tmp, into = c('scenario', 't1', 't2'), sep = '~') %>%
    select(-t1, -t2) ->
    stitched_tgav2

ggplot(data = stitched_tgav2, aes(x= year, y = value, color = stitching_id)) +
    geom_line(size=0.2) +
  geom_line(data = hector_tgav, aes(x = year, y=value), color='black', size = 1.1) + 
  geom_line(data = py$target_hector, aes(x = year, y=value), color='blue', size = 0.7) + 
  ylab('degC') +
  ggtitle('global average temperature anomaly - Ensemble 2')+ 
  theme(legend.position="bottom") +
  theme(legend.title = element_text( size=4), legend.text=element_text(size=4))

```

if you're really pressed for computing time, you can pair this with pattern scaling and get the mean spatial field. But we can do internal variability at the gridded multivariate scale, so we do.

# stitch gridded
Using the above recipes, stitches can produce new gridded netcdf files for each recipe, for multiple variables. 
Creating multiple netcdfs of monthly data is slower than focusing on GSAT. Still much faster than an ESM, and once the recipes are set, very parallelizable. But for tractability, we are just going to stitch the gridded netcdfs for one ensemble member.

```{python}
# Just do one realization
recipe = my_recipes[my_recipes['stitching_id'] == 'RCP  Standard-RCP-4.5~hectorUI1~1'].reset_index(drop=True).copy()

```


We must specify a directory for the netcdfs to be saved to. We will use simply do it directly here.
On my computer, constructing these 4 netcdfs took ~2-3 minutes.

```{python}
stitches.gridded_stitching(out_dir='data', rp =recipe)
```


# plot gridded

- we will do maps of all 4 variables at a specified month and year
- we will do a time series of each variable in a specific grid cell

```{r}
# pull all netcdfs:
files <- list.files('data', pattern  = '.nc', full.names = TRUE)

#subset to just the ESM of interest:
files <- files[grepl(py$esm, files)]

# subset to just the Hector scenario read in above:
files <- files[grepl(hector_exp, files)]

print(files)

map_time <- '2100-12-31'
```

- I like python xarray functions for reading in and plotting netcdf data better, but here is some code in R that twill result in a long data frame for each variable. Lot slower than python functions so set to `eval=FALSE`

## r netcdf read in for r plotting
```{r, eval=FALSE}
library(netcdf4)
# function to convert dates
  ##TODO you'll have to adjust this depending on your calendar and units in climate data:
  # days since 1850-01-01 00:00:00, 365 day year
  convert_time <- function(time, reference_year, nc_start_year){
    time %>%
      dplyr::mutate(time_index = as.integer(row.names(.)) - 1 + 12*(nc_start_year - reference_year),
             month = floor(time_index %% 12) + 1,
             year = floor(time_index/ 12) + reference_year,
             time_id = paste0(month, '~', year )) %>%
      dplyr::select(time, time_id)
  }

# get the data read in and reshaped
for (v in py$vars1){
  nc_name <- files[grepl(v, files)]
  print(nc_name)
  
  # open file
  ncfile1 <- ncdf4::nc_open(nc_name)
  
  # pull off lon/lat/time info
  lat1 <- ncdf4::ncvar_get(ncfile1,"lat")
  lon1 <- ncdf4::ncvar_get(ncfile1,"lon")
  time1 <- ncdf4::ncvar_get(ncfile1, "time") # units: days since 1850-01-31 00:00:00
  
  # get the var data as a [nlat*nlon, ntime] unlabled matrix
  var_data <- t(rbind(matrix(aperm(ncdf4::ncvar_get(ncfile1,v),c(3,1,2)),length(time1),length(lat1)*length(lon1))))
  ncdf4:nc_close()
  
  # add time labels
  colnames(var_data) <- convert_time(time = data.frame(time=time1),
                                 reference_year = 1850,
                                 nc_start_year = 1850)$time_id
  
  # add lat/lon labels and reshape to long format
  grid <- expand.grid(list(lon=lon1,lat=lat1))
  assign(paste0(v, 'data'),
          cbind(grid, var_data) %>%
      tidyr::gather(time, value, -lon, -lat) %>%
      tidyr::separate(time, into = c('month', 'year'), sep = '~') %>%
      dplyr::mutate(year = as.integer(year),
             month=as.integer(month)))
  rm(var_data)
  rm(time1)
  rm(lon1)
  rm(lat1)
  rm(grid)
}

```

# python maps and time series

It's a lot easier to use python to load in the netcdfs and select a grid cell or a time slice, save those off as simpler data frames than a full netcdf, and plot in R.'



## tas 
TODO just make it a function
```{python}
py_files = pd.DataFrame(r.files, columns = ['file_name'])

# load the netcdf:
f = py_files.loc[py_files['file_name'].str.contains('tas')].values[0]
tas_nc = xr.open_dataset(f[0])

# time slice map:
tas_time_slice = tas_nc.sel(time = r.map_time).copy()


# time series for single grid cell:
# lon and lat values for a grid cell near College Park, MD, home of JGCRI:
cp_lat = 38.9897
cp_lon = 180 + 76.9378 # this is probably not actually right 

# lat and lon coordinates closest
abslat = np.abs(tas_nc.lat - cp_lat)
abslon = np.abs(tas_nc.lon-cp_lon)
c = np.maximum(abslon, abslat)
([lon_loc], [lat_loc]) = np.where(c == np.min(c))
lon_grid = tas_nc.lon[lon_loc]
lat_grid = tas_nc.lat[lat_loc]

tas_grid_slice = tas_nc.sel(lon = lon_grid, lat=lat_grid,
                         time = slice('1850-01-01', '2099-12-31')).copy()
del(tas_nc)
```

Take a look at this data in R:
```{r}
# map data:
print(py$tas_time_slice$tas)
tas_val <- py$tas_time_slice$tas$values
tas_lon <- py$tas_time_slice$lon$values
tas_lat <- py$tas_time_slice$lat$values
grid <- expand.grid(list(lon=tas_lon,lat=tas_lat))
tas <- cbind(grid, 
             value=t(matrix(aperm(tas_val,c(2,1)), 1,length(tas_lat)*length(tas_lon))))

as.character(py$tas_time_slice$time) %>%
  substr(.,  37, 43) ->
  tas_time
  
ggplot(tas, aes(x = lon, y = lat, color=value, fill = value)) + geom_tile() +
  ggtitle(paste(tas_time, 'tas (K)'))


# grid time series data:
print(py$tas_grid_slice$tas)
cp_tas <- data.frame(value = py$tas_grid_slice$tas$values)

data.frame(time = as.character(py$tas_grid_slice$time$values)) %>%
  separate(time, into = c('year', 'month', 'day'), sep = '-') %>%
  select(-day) %>%
  mutate(year = as.integer(year),
         month = as.integer(month),
         time_index = as.integer(row.names(.))) ->
  cp_time

ggplot(cbind(cp_time, cp_tas), aes(x = time_index, y = value)) + geom_line() +
  ggtitle('College Park monthly tas, 1850-2100 (K)') 

ggplot(cbind(cp_time, cp_tas) %>% filter(year >= 2021), aes(x = time_index, y = value)) + geom_line() +
  ggtitle('College Park monthly tas, 2021-2100 (K)') 

```





## pr
```{python}
py_files = pd.DataFrame(r.files, columns = ['file_name'])

# load the netcdf:
f = py_files.loc[py_files['file_name'].str.contains('pr')].values[0]
pr_nc = xr.open_dataset(f[0])

# time slice map:
pr_time_slice = pr_nc.sel(time = r.map_time).copy()


# time series for single grid cell:
# lon and lat values for a grid cell near College Park, MD, home of JGCRI:
cp_lat = 38.9897
cp_lon = 180 + 76.9378 # this is probably not actually right 

# lat and lon coordinates closest
abslat = np.abs(pr_nc.lat - cp_lat)
abslon = np.abs(pr_nc.lon-cp_lon)
c = np.maximum(abslon, abslat)
([lon_loc], [lat_loc]) = np.where(c == np.min(c))
lon_grid = pr_nc.lon[lon_loc]
lat_grid = pr_nc.lat[lat_loc]

pr_grid_slice = pr_nc.sel(lon = lon_grid, lat=lat_grid,
                         time = slice('1850-01-01', '2099-12-31')).copy()
del(pr_nc)
```

Take a look at this data in R:
```{r}
# map data:
print(py$pr_time_slice$pr)
pr_val <- py$pr_time_slice$pr$values
pr_lon <- py$pr_time_slice$lon$values
pr_lat <- py$pr_time_slice$lat$values
grid <- expand.grid(list(lon=pr_lon,lat=pr_lat))
pr <- cbind(grid, 
             value=t(matrix(aperm(pr_val,c(2,1)), 1,length(pr_lat)*length(pr_lon))))

as.character(py$pr_time_slice$time) %>%
  substr(.,  37, 43) ->
  pr_time
  
ggplot(pr, aes(x = lon, y = lat, color=value, fill = value)) + geom_tile() +
  ggtitle(paste(pr_time, 'pr (kg m-2 s-1)'))


# grid time series data:
print(py$pr_grid_slice$pr)
cp_pr <- data.frame(value = py$pr_grid_slice$pr$values)

data.frame(time = as.character(py$pr_grid_slice$time$values)) %>%
  separate(time, into = c('year', 'month', 'day'), sep = '-') %>%
  select(-day) %>%
  mutate(year = as.integer(year),
         month = as.integer(month),
         time_index = as.integer(row.names(.))) ->
  cp_time

ggplot(cbind(cp_time, cp_pr), aes(x = time_index, y = value)) + geom_line() +
  ggtitle('College Park monthly pr (kg m-2 s-1), 1850-2100') 

ggplot(cbind(cp_time, cp_pr) %>% filter(year >= 2021), aes(x = time_index, y = value)) + geom_line() +
  ggtitle('College Park monthly pr (kg m-2 s-1), 2021-2100') 

```

## hurs
```{python}
py_files = pd.DataFrame(r.files, columns = ['file_name'])

# load the netcdf:
f = py_files.loc[py_files['file_name'].str.contains('hurs')].values[0]
hurs_nc = xr.open_dataset(f[0])

# time slice map:
hurs_time_slice = hurs_nc.sel(time = r.map_time).copy()


# time series for single grid cell:
# lon and lat values for a grid cell near College Park, MD, home of JGCRI:
cp_lat = 38.9897
cp_lon = 180 + 76.9378 # this is probably not actually right 

# lat and lon coordinates closest
abslat = np.abs(hurs_nc.lat - cp_lat)
abslon = np.abs(hurs_nc.lon-cp_lon)
c = np.maximum(abslon, abslat)
([lon_loc], [lat_loc]) = np.where(c == np.min(c))
lon_grid = hurs_nc.lon[lon_loc]
lat_grid = hurs_nc.lat[lat_loc]

hurs_grid_slice = hurs_nc.sel(lon = lon_grid, lat=lat_grid,
                         time = slice('1850-01-01', '2099-12-31')).copy()
del(hurs_nc)
```

Take a look at this data in R:
```{r}
# map data:
print(py$hurs_time_slice$hurs)
hurs_val <- py$hurs_time_slice$hurs$values
hurs_lon <- py$hurs_time_slice$lon$values
hurs_lat <- py$hurs_time_slice$lat$values
grid <- expand.grid(list(lon=pr_lon,lat=pr_lat))
hurs <- cbind(grid, 
             value=t(matrix(aperm(hurs_val,c(2,1)), 1,length(hurs_lat)*length(hurs_lon))))

as.character(py$hurs_time_slice$time) %>%
  substr(.,  37, 43) ->
  hurs_time
  
ggplot(hurs, aes(x = lon, y = lat, color=value, fill = value)) + geom_tile() +
  ggtitle(paste(hurs_time, 'hurs (%)'))


# grid time series data:
print(py$hurs_grid_slice$hurs)
cp_hurs <- data.frame(value = py$hurs_grid_slice$hurs$values)

data.frame(time = as.character(py$hurs_grid_slice$time$values)) %>%
  separate(time, into = c('year', 'month', 'day'), sep = '-') %>%
  select(-day) %>%
  mutate(year = as.integer(year),
         month = as.integer(month),
         time_index = as.integer(row.names(.))) ->
  cp_time

ggplot(cbind(cp_time, cp_hurs), aes(x = time_index, y = value)) + geom_line() +
  ggtitle('College Park monthly hurs (%), 1850-2100') 

ggplot(cbind(cp_time, cp_hurs) %>% filter(year >= 2021), aes(x = time_index, y = value)) + geom_line() +
  ggtitle('College Park monthly hurs (%), 2021-2100') 

```

## psl

sea level pressure can't have a CP time series
```{python}
py_files = pd.DataFrame(r.files, columns = ['file_name'])

# load the netcdf:
f = py_files.loc[py_files['file_name'].str.contains('psl')].values[0]
psl_nc = xr.open_dataset(f[0])

# time slice map:
psl_time_slice = psl_nc.sel(time = r.map_time).copy()
del(psl_nc)

```

Take a look at this data in R:
```{r}
# map data:
print(py$psl_time_slice$psl)
psl_val <- py$psl_time_slice$psl$values
psl_lon <- py$psl_time_slice$lon$values
psl_lat <- py$psl_time_slice$lat$values
grid <- expand.grid(list(lon=psl_lon,lat=psl_lat))
psl <- cbind(grid, 
             value=t(matrix(aperm(psl_val,c(2,1)), 1,length(psl_lat)*length(psl_lon))))

as.character(py$psl_time_slice$time) %>%
  substr(.,  37, 43) ->
  pr_time
  
ggplot(psl, aes(x = lon, y = lat, color=value, fill = value)) + geom_tile() +
  ggtitle(paste(pr_time, 'psl (Pa)'))

```
